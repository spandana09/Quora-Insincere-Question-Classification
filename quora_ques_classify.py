# -*- coding: utf-8 -*-
"""quora_ques_classify.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vr069I80Ji-CTWBwykOReA-jAlmTwsCO
"""

!pip install kaggle

!mkdir ~/.kaggle

from google.colab import drive
drive.mount('/content/gdrive')

!cp /content/gdrive/My\ Drive/kaggle.json ~/.kaggle/kaggle.json

!kaggle competitions download -c quora-insincere-questions-classification -p /content

!unzip \*.zip

import pandas as pd
df_train=pd.read_csv('/content/train.csv')
df_test=pd.read_csv('/content/test.csv')

df_train.head()

df_test.head()

df_train['target'].value_counts()

#preprocessing the dataset
import nltk
import re
from nltk import word_tokenize
nltk.download('stopwords')
from nltk.corpus import stopwords
nltk.download('punkt')

s_w = set(stopwords.words('english'))

def stop_words(string):
    return ' '.join([x for x in word_tokenize(string) if not x in s_w])

def clean_strings(string1):
  new_string=re.sub('[\n]',' ',string1) #removing newline character
  new_string=re.sub('[^a-zA-Z]',' ',new_string) #removing non alpha characters
  new_string=stop_words(new_string) #removing stopwords
  return new_string

questions=[]
targets=[]
for i in range(len(df_train)):
  targets.append(df_train['target'][i])
  questions.append(clean_strings(df_train['question_text'][i]))

test_question=[]
for i in range(len(df_test)):
  test_question.append(clean_strings(df_train['question_text'][i]))

#splitting the data into train and test sets 
from sklearn.model_selection import train_test_split
train_questions, test_questions, train_targets, test_targets= train_test_split(questions, targets, test_size=0.2, shuffle= True)

print('Data: ',len(df_train))
print('Training examples: ',len(train_questions))
print('Test examples: ',len(test_questions))

#data tokenizing and padding
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences


embed_dim = 300
max_len = 100
trunc_type='post'
pad_type='post'
oov_tok = "<OOV>"
voc_len=49999


tokenizer = Tokenizer(num_words=voc_len+1,oov_token=oov_tok)
tokenizer.fit_on_texts(questions)

word_index = tokenizer.word_index
voc_size=len(word_index)
print('Original Size of Vocabulary: ',voc_size)

word_index = {e:i for e,i in word_index.items() if i <= voc_len+1} #Reducing the Size of Vocabulary
print('New Size of Vocabulary: ',len(word_index))

seq = tokenizer.texts_to_sequences(questions)
pad = pad_sequences(seq, maxlen=max_len, padding=pad_type, truncating=trunc_type)

train_seq = tokenizer.texts_to_sequences(train_questions)
train_pad = pad_sequences(train_seq, maxlen=max_len, padding=pad_type, truncating=trunc_type)

test_seq = tokenizer.texts_to_sequences(test_questions)
test_pad = pad_sequences(test_seq, maxlen=max_len, padding=pad_type, truncating=trunc_type)

test_sequences = tokenizer.texts_to_sequences(test_question)
test_padd = pad_sequences(test_sequences, maxlen=max_len, padding=pad_type, truncating=trunc_type)

train_targets=np.expand_dims(train_targets, axis=1)
test_targets=np.expand_dims(test_targets, axis=1)

"""**BIDIRECTIONAL LSTM**"""

#Bidirectional LSTM model building
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(voc_len+1, embed_dim, input_length=max_len),
    tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(64,return_sequences=True)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, mode='auto')
model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])
num_epoch = 5
his=model.fit(train_pad, train_targets, epochs=num_epoch, batch_size=512, validation_data=(test_pad,test_targets),callbacks=[reduce],verbose=1)

#Determining the optimal threshold value
pred = model.predict(test_pad, batch_size=512,verbose=1)
from sklearn import metrics
for th in np.arange(0.1, 0.501, 0.05):
    th = np.round(th, 2)
    print("F1 score at threshold {0} is {1}".format(th, metrics.f1_score(test_targets, (pred>th).astype(int))))

# From above, F1 score is highest at a threshold of 0.25
pred_test_targets=(pred>0.25).astype(int)

from sklearn.metrics import classification_report
print(classification_report(test_targets,pred_test_targets))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(test_targets, pred_test_targets))
cm = confusion_matrix(test_targets, pred_test_targets)

from mlxtend.plotting import plot_confusion_matrix
fig, ax = plot_confusion_matrix(conf_mat=cm ,  figsize=(5, 5))

import matplotlib.pyplot as plt
plt.show()

#predicting the labels of test data by passing the test data through the model
pred = model.predict(test_padd, batch_size=512,verbose=1)
pred_test_targets = (pred>0.25).astype(int)

df_output = pd.DataFrame({'qid':df_test['qid'].values})
df_output['prediction'] = pred_test_targets
df_output.to_csv('submission.csv', index=False)

out_df=pd.read_csv('submission.csv')
out_df['prediction'].value_counts()